{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba1ab1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch CUDA version: 11.8\n",
      "Is CUDA available? True\n",
      "Using device: cuda\n",
      "NVIDIA GeForce RTX 2060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\K1taru\\AppData\\Roaming\\Python\\Python313\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\K1taru\\AppData\\Roaming\\Python\\Python313\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/300, Loss: 7.3255\n",
      "Batch 2/300, Loss: 5.0591\n",
      "Batch 3/300, Loss: 3.7613\n",
      "Batch 4/300, Loss: 3.8443\n",
      "Batch 5/300, Loss: 4.4730\n",
      "Batch 6/300, Loss: 4.9915\n",
      "Batch 7/300, Loss: 5.0083\n",
      "Batch 8/300, Loss: 4.6007\n",
      "Batch 9/300, Loss: 4.0209\n",
      "Batch 10/300, Loss: 3.3588\n",
      "Batch 11/300, Loss: 3.0868\n",
      "Batch 12/300, Loss: 3.0280\n",
      "Batch 13/300, Loss: 4.3350\n",
      "Batch 14/300, Loss: 3.7617\n",
      "Batch 15/300, Loss: 3.9345\n",
      "Batch 16/300, Loss: 3.3660\n",
      "Batch 17/300, Loss: 2.5566\n",
      "Batch 18/300, Loss: 2.8947\n",
      "Batch 19/300, Loss: 2.8450\n",
      "Batch 20/300, Loss: 3.3889\n",
      "Batch 21/300, Loss: 4.3536\n",
      "Batch 22/300, Loss: 4.3384\n",
      "Batch 23/300, Loss: 3.5160\n",
      "Batch 24/300, Loss: 6.9500\n",
      "Batch 25/300, Loss: 6.3305\n",
      "Batch 26/300, Loss: 5.3075\n",
      "Batch 27/300, Loss: 3.4012\n",
      "Batch 28/300, Loss: 3.7036\n",
      "Batch 29/300, Loss: 4.4929\n",
      "Batch 30/300, Loss: 4.2565\n",
      "Batch 31/300, Loss: 4.8879\n",
      "Batch 32/300, Loss: 5.2484\n",
      "Batch 33/300, Loss: 4.3445\n",
      "Batch 34/300, Loss: 3.9706\n",
      "Batch 35/300, Loss: 4.2398\n",
      "Batch 36/300, Loss: 5.5662\n",
      "Batch 37/300, Loss: 5.5916\n",
      "Batch 38/300, Loss: 8.0109\n",
      "Batch 39/300, Loss: 5.1235\n",
      "Batch 40/300, Loss: 4.3706\n",
      "Batch 41/300, Loss: 2.7050\n",
      "Batch 42/300, Loss: 4.0042\n",
      "Batch 43/300, Loss: 5.1910\n",
      "Batch 44/300, Loss: 6.5366\n",
      "Batch 45/300, Loss: 3.5855\n",
      "Batch 46/300, Loss: 4.5435\n",
      "Batch 47/300, Loss: 3.7535\n",
      "Batch 48/300, Loss: 4.5894\n",
      "Batch 49/300, Loss: 5.3346\n",
      "Batch 50/300, Loss: 3.9940\n",
      "Batch 51/300, Loss: 4.8929\n",
      "Batch 52/300, Loss: 6.0825\n",
      "Batch 53/300, Loss: 3.8784\n",
      "Batch 54/300, Loss: 3.4285\n",
      "Batch 55/300, Loss: 4.6549\n",
      "Batch 56/300, Loss: 5.7554\n",
      "Batch 57/300, Loss: 4.7305\n",
      "Batch 58/300, Loss: 6.1504\n",
      "Batch 59/300, Loss: 7.4680\n",
      "Batch 60/300, Loss: 6.1833\n",
      "Batch 61/300, Loss: 7.3299\n",
      "Batch 62/300, Loss: 6.1642\n",
      "Batch 63/300, Loss: 4.4376\n",
      "Batch 64/300, Loss: 4.2467\n",
      "Batch 65/300, Loss: 5.4962\n",
      "Batch 66/300, Loss: 6.4560\n",
      "Batch 67/300, Loss: 5.3004\n",
      "Batch 68/300, Loss: 7.6390\n",
      "Batch 69/300, Loss: 6.8497\n",
      "Batch 70/300, Loss: 6.0705\n",
      "Batch 71/300, Loss: 4.1023\n",
      "Batch 72/300, Loss: 5.8268\n",
      "Batch 73/300, Loss: 7.3427\n",
      "Batch 74/300, Loss: 5.8737\n",
      "Batch 75/300, Loss: 7.3333\n",
      "Batch 76/300, Loss: 9.0639\n",
      "Batch 77/300, Loss: 6.7739\n",
      "Batch 78/300, Loss: 5.3006\n",
      "Batch 79/300, Loss: 6.2252\n",
      "Batch 80/300, Loss: 4.2862\n",
      "Batch 81/300, Loss: 4.7129\n",
      "Batch 82/300, Loss: 5.4860\n",
      "Batch 83/300, Loss: 3.5954\n",
      "Batch 84/300, Loss: 5.9852\n",
      "Batch 85/300, Loss: 6.4157\n",
      "Batch 86/300, Loss: 6.4507\n",
      "Batch 87/300, Loss: 8.0871\n",
      "Batch 88/300, Loss: 5.4335\n",
      "Batch 89/300, Loss: 3.4742\n",
      "Batch 90/300, Loss: 5.7647\n",
      "Batch 91/300, Loss: 3.5029\n",
      "Batch 92/300, Loss: 4.8096\n",
      "Batch 93/300, Loss: 5.4729\n",
      "Batch 94/300, Loss: 6.0824\n",
      "Batch 95/300, Loss: 6.5341\n",
      "Batch 96/300, Loss: 3.8590\n",
      "Batch 97/300, Loss: 4.6900\n",
      "Batch 98/300, Loss: 5.4164\n",
      "Batch 99/300, Loss: 4.1931\n",
      "Batch 100/300, Loss: 3.5574\n",
      "Batch 101/300, Loss: 3.1279\n",
      "Batch 102/300, Loss: 3.4245\n",
      "Batch 103/300, Loss: 3.9215\n",
      "Batch 104/300, Loss: 3.9215\n",
      "Batch 105/300, Loss: 3.4917\n",
      "Batch 106/300, Loss: 3.0700\n",
      "Batch 107/300, Loss: 4.6044\n",
      "Batch 108/300, Loss: 4.6908\n",
      "Batch 109/300, Loss: 3.1748\n",
      "Batch 110/300, Loss: 4.9983\n",
      "Batch 111/300, Loss: 5.4700\n",
      "Batch 112/300, Loss: 4.6437\n",
      "Batch 113/300, Loss: 5.7615\n",
      "Batch 114/300, Loss: 4.0716\n",
      "Batch 115/300, Loss: 5.0537\n",
      "Batch 116/300, Loss: 4.3194\n",
      "Batch 117/300, Loss: 3.9930\n",
      "Batch 118/300, Loss: 6.0825\n",
      "Batch 119/300, Loss: 6.0533\n",
      "Batch 120/300, Loss: 5.1304\n",
      "Batch 121/300, Loss: 6.6121\n",
      "Batch 122/300, Loss: 5.8629\n",
      "Batch 123/300, Loss: 3.1406\n",
      "Batch 124/300, Loss: 4.2786\n",
      "Batch 125/300, Loss: 3.0924\n",
      "Batch 126/300, Loss: 4.9567\n",
      "Batch 127/300, Loss: 4.8303\n",
      "Batch 128/300, Loss: 5.3918\n",
      "Batch 129/300, Loss: 3.6063\n",
      "Batch 130/300, Loss: 4.2593\n",
      "Batch 131/300, Loss: 4.0536\n",
      "Batch 132/300, Loss: 3.4002\n",
      "Batch 133/300, Loss: 3.5606\n",
      "Batch 134/300, Loss: 3.1090\n",
      "Batch 135/300, Loss: 2.9640\n",
      "Batch 136/300, Loss: 2.5307\n",
      "Batch 137/300, Loss: 4.6650\n",
      "Batch 138/300, Loss: 3.7196\n",
      "Batch 139/300, Loss: 4.0276\n",
      "Batch 140/300, Loss: 3.3323\n",
      "Batch 141/300, Loss: 4.9886\n",
      "Batch 142/300, Loss: 4.2935\n",
      "Batch 143/300, Loss: 5.0741\n",
      "Batch 144/300, Loss: 3.7422\n",
      "Batch 145/300, Loss: 4.4931\n",
      "Batch 146/300, Loss: 3.3962\n",
      "Batch 147/300, Loss: 4.1425\n",
      "Batch 148/300, Loss: 3.9816\n",
      "Batch 149/300, Loss: 4.2993\n",
      "Batch 150/300, Loss: 6.0967\n",
      "Batch 151/300, Loss: 3.2325\n",
      "Batch 152/300, Loss: 4.6156\n",
      "Batch 153/300, Loss: 3.6385\n",
      "Batch 154/300, Loss: 4.6562\n",
      "Batch 155/300, Loss: 5.3302\n",
      "Batch 156/300, Loss: 4.7503\n",
      "Batch 157/300, Loss: 6.4925\n",
      "Batch 158/300, Loss: 8.1487\n",
      "Batch 159/300, Loss: 4.4742\n",
      "Batch 160/300, Loss: 4.8345\n",
      "Batch 161/300, Loss: 5.8676\n",
      "Batch 162/300, Loss: 4.2274\n",
      "Batch 163/300, Loss: 3.7439\n",
      "Batch 164/300, Loss: 5.2244\n",
      "Batch 165/300, Loss: 4.8288\n",
      "Batch 166/300, Loss: 4.4517\n",
      "Batch 167/300, Loss: 3.6861\n",
      "Batch 168/300, Loss: 4.7537\n",
      "Batch 169/300, Loss: 4.4439\n",
      "Batch 170/300, Loss: 5.3031\n",
      "Batch 171/300, Loss: 6.9109\n",
      "Batch 172/300, Loss: 5.5482\n",
      "Batch 173/300, Loss: 4.3663\n",
      "Batch 174/300, Loss: 4.0731\n",
      "Batch 175/300, Loss: 4.8093\n",
      "Batch 176/300, Loss: 5.1782\n",
      "Batch 177/300, Loss: 5.1597\n",
      "Batch 178/300, Loss: 2.6026\n",
      "Batch 179/300, Loss: 3.4589\n",
      "Batch 180/300, Loss: 3.5622\n",
      "Batch 181/300, Loss: 4.3217\n",
      "Batch 182/300, Loss: 4.0330\n",
      "Batch 183/300, Loss: 5.1839\n",
      "Batch 184/300, Loss: 4.8796\n",
      "Batch 185/300, Loss: 4.0625\n",
      "Batch 186/300, Loss: 4.5734\n",
      "Batch 187/300, Loss: 3.8694\n",
      "Batch 188/300, Loss: 4.0289\n",
      "Batch 189/300, Loss: 4.1922\n",
      "Batch 190/300, Loss: 3.9778\n",
      "Batch 191/300, Loss: 3.3140\n",
      "Batch 192/300, Loss: 3.7616\n",
      "Batch 193/300, Loss: 3.6820\n",
      "Batch 194/300, Loss: 4.9714\n",
      "Batch 195/300, Loss: 6.4403\n",
      "Batch 196/300, Loss: 5.7769\n",
      "Batch 197/300, Loss: 4.8963\n",
      "Batch 198/300, Loss: 5.2925\n",
      "Batch 199/300, Loss: 3.8630\n",
      "Batch 200/300, Loss: 3.2769\n",
      "Batch 201/300, Loss: 3.4786\n",
      "Batch 202/300, Loss: 3.4380\n",
      "Batch 203/300, Loss: 2.8971\n",
      "Batch 204/300, Loss: 2.5676\n",
      "Batch 205/300, Loss: 2.7286\n",
      "Batch 206/300, Loss: 3.9968\n",
      "Batch 207/300, Loss: 4.1866\n",
      "Batch 208/300, Loss: 4.0684\n",
      "Batch 209/300, Loss: 3.7954\n",
      "Batch 210/300, Loss: 4.1834\n",
      "Batch 211/300, Loss: 3.5803\n",
      "Batch 212/300, Loss: 4.1239\n",
      "Batch 213/300, Loss: 4.5232\n",
      "Batch 214/300, Loss: 3.2712\n",
      "Batch 215/300, Loss: 5.0433\n",
      "Batch 216/300, Loss: 6.5254\n",
      "Batch 217/300, Loss: 4.8526\n",
      "Batch 218/300, Loss: 5.0218\n",
      "Batch 219/300, Loss: 3.4648\n",
      "Batch 220/300, Loss: 4.0218\n",
      "Batch 221/300, Loss: 3.0436\n",
      "Batch 222/300, Loss: 4.0461\n",
      "Batch 223/300, Loss: 3.6835\n",
      "Batch 224/300, Loss: 4.7864\n",
      "Batch 225/300, Loss: 5.3273\n",
      "Batch 226/300, Loss: 4.9005\n",
      "Batch 227/300, Loss: 6.4493\n",
      "Batch 228/300, Loss: 6.5532\n",
      "Batch 229/300, Loss: 5.7140\n",
      "Batch 230/300, Loss: 6.4132\n",
      "Batch 231/300, Loss: 4.5004\n",
      "Batch 232/300, Loss: 5.8257\n",
      "Batch 233/300, Loss: 7.3508\n",
      "Batch 234/300, Loss: 4.4791\n",
      "Batch 235/300, Loss: 6.7368\n",
      "Batch 236/300, Loss: 4.4232\n",
      "Batch 237/300, Loss: 3.9946\n",
      "Batch 238/300, Loss: 3.7149\n",
      "Batch 239/300, Loss: 4.8134\n",
      "Batch 240/300, Loss: 2.7965\n",
      "Batch 241/300, Loss: 4.1633\n",
      "Batch 242/300, Loss: 3.9947\n",
      "Batch 243/300, Loss: 5.1086\n",
      "Batch 244/300, Loss: 5.0179\n",
      "Batch 245/300, Loss: 4.1728\n",
      "Batch 246/300, Loss: 5.2069\n",
      "Batch 247/300, Loss: 4.9977\n",
      "Batch 248/300, Loss: 3.8502\n",
      "Batch 249/300, Loss: 3.6514\n",
      "Batch 250/300, Loss: 2.6008\n",
      "Batch 251/300, Loss: 2.7853\n",
      "Batch 252/300, Loss: 4.4569\n",
      "Batch 253/300, Loss: 3.2107\n",
      "Batch 254/300, Loss: 4.9306\n",
      "Batch 255/300, Loss: 5.1428\n",
      "Batch 256/300, Loss: 4.8453\n",
      "Batch 257/300, Loss: 5.6193\n",
      "Batch 258/300, Loss: 3.7074\n",
      "Batch 259/300, Loss: 3.2287\n",
      "Batch 260/300, Loss: 5.9902\n",
      "Batch 261/300, Loss: 5.5425\n",
      "Batch 262/300, Loss: 6.0616\n",
      "Batch 263/300, Loss: 6.4147\n",
      "Batch 264/300, Loss: 6.9115\n",
      "Batch 265/300, Loss: 4.0105\n",
      "Batch 266/300, Loss: 5.3030\n",
      "Batch 267/300, Loss: 4.4175\n",
      "Batch 268/300, Loss: 4.3272\n",
      "Batch 269/300, Loss: 3.9466\n",
      "Batch 270/300, Loss: 4.1724\n",
      "Batch 271/300, Loss: 3.8021\n",
      "Batch 272/300, Loss: 5.7035\n",
      "Batch 273/300, Loss: 6.4117\n",
      "Batch 274/300, Loss: 3.6790\n",
      "Batch 275/300, Loss: 4.4872\n",
      "Batch 276/300, Loss: 4.9056\n",
      "Batch 277/300, Loss: 4.9984\n",
      "Batch 278/300, Loss: 4.1230\n",
      "Batch 279/300, Loss: 3.5796\n",
      "Batch 280/300, Loss: 4.8818\n",
      "Batch 281/300, Loss: 4.3617\n",
      "Batch 282/300, Loss: 4.3123\n",
      "Batch 283/300, Loss: 4.2549\n",
      "Batch 284/300, Loss: 4.1781\n",
      "Batch 285/300, Loss: 4.0313\n",
      "Batch 286/300, Loss: 3.8977\n",
      "Batch 287/300, Loss: 5.8954\n",
      "Batch 288/300, Loss: 3.9237\n",
      "Batch 289/300, Loss: 3.3684\n",
      "Batch 290/300, Loss: 2.9481\n",
      "Batch 291/300, Loss: 3.6407\n",
      "Batch 292/300, Loss: 3.2407\n",
      "Batch 293/300, Loss: 3.7173\n",
      "Batch 294/300, Loss: 3.9321\n",
      "Batch 295/300, Loss: 5.5205\n",
      "Batch 296/300, Loss: 3.5103\n",
      "Batch 297/300, Loss: 3.6565\n",
      "Batch 298/300, Loss: 3.5999\n",
      "Batch 299/300, Loss: 4.8317\n",
      "Batch 300/300, Loss: 4.7557\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import models\n",
    "\n",
    "# --- Check CUDA ---\n",
    "print(\"PyTorch CUDA version:\", torch.version.cuda)\n",
    "print(\"Is CUDA available?\", torch.cuda.is_available())\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "print(torch.cuda.get_device_name(0))  # Should print 'NVIDIA GeForce RTX 2060'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
