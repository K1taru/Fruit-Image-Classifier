{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd6ffd63",
   "metadata": {},
   "source": [
    "## Fruit Image Classifier - Part 1: Imports & Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4085a153",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Fix random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# src imports\n",
    "from gpu_utils import CheckGPU, CheckCUDA, CheckGPUBrief\n",
    "from dataset_counter import CountDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f3294f",
   "metadata": {},
   "source": [
    "#### Detect GPU Available, Details, Cuda, and cuDNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b25d3114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "üñ•Ô∏è  GPU INFORMATION\n",
      "==================================================\n",
      "‚úÖ GPU Detected         : NVIDIA GeForce RTX 2060\n",
      "   ‚Ä¢ Device ID          : 0\n",
      "   ‚Ä¢ Compute Capability : 7.5\n",
      "   ‚Ä¢ Multiprocessors    : 30\n",
      "   ‚Ä¢ Total VRAM         : 6.00 GB\n",
      "   ‚Ä¢ VRAM Allocated     : 0.00 GB\n",
      "   ‚Ä¢ VRAM Reserved      : 0.00 GB\n",
      "   ‚Ä¢ Active Device      : cuda\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "‚ö° CUDA / PYTORCH INFORMATION\n",
      "==================================================\n",
      "‚úÖ CUDA Available       : True\n",
      "   ‚Ä¢ PyTorch CUDA Ver.  : 11.8\n",
      "   ‚Ä¢ PyTorch Version    : 2.7.1+cu118\n",
      "‚úÖ cuDNN Version        : 90100\n",
      "   ‚Ä¢ CUDA Device Count  : 1\n",
      "   ‚Ä¢ Device 0 Name     : NVIDIA GeForce RTX 2060\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# From \n",
    "CheckGPU()\n",
    "CheckCUDA()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e219a799",
   "metadata": {},
   "source": [
    "### Define dataset path and categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d0a958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä DATASET SUMMARY\n",
      "===============================================================\n",
      "Class       count       ratio     needed        size_mb\n",
      "===============================================================\n",
      "Apple       19515      1.0000          0      816.94 MB\n",
      "Banana      11612      1.6806       7903      223.78 MB\n",
      "Grapes       2198      8.8785      17317     6605.82 MB\n",
      "Mango        2505      7.7904      17010      191.21 MB\n",
      "Orange        232     84.1164      19283       29.25 MB\n",
      "===============================================================\n",
      "total_images  36062\n",
      "max_class_count  19515\n",
      "total_size_mb                                   7867.01 MB\n",
      "===============================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "DATASET_DIR = \"../dataset\"\n",
    "\n",
    "DATASET_INFO = CountDataset(DATASET_DIR)\n",
    "'''\n",
    "# Example:\n",
    "DATASET_INFO = {\n",
    "    \"Apple\":  {\"count\": 19515, \"ratio\":  1.0000, \"needed\":     0, \"size_mb\":  816.94},\n",
    "    \"Banana\": {\"count\": 11612, \"ratio\":  1.6806, \"needed\":  7903, \"size_mb\":  223.78},\n",
    "    \"Grapes\": {\"count\":  2198, \"ratio\":  8.8785, \"needed\": 17317, \"size_mb\": 6605.82},\n",
    "    \"Mango\":  {\"count\":  2505, \"ratio\":  7.7904, \"needed\": 17010, \"size_mb\":  191.21},\n",
    "    \"Orange\": {\"count\":   232, \"ratio\": 84.1164, \"needed\": 19283, \"size_mb\":   29.25},\n",
    "\n",
    "    \"total_images\": 36062,\n",
    "    \"max_class_count\": 19515,\n",
    "    \"total_size_mb\": 7867.01\n",
    "}\n",
    "'''\n",
    "\n",
    "\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830a63b3",
   "metadata": {},
   "source": [
    "### Load and preprocess images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e12cea03",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SPLITS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[43mSPLITS\u001b[49m:\n\u001b[32m      2\u001b[39m     split_data = []\n\u001b[32m      3\u001b[39m     split_labels = []\n",
      "\u001b[31mNameError\u001b[39m: name 'SPLITS' is not defined"
     ]
    }
   ],
   "source": [
    "for split in SPLITS:\n",
    "    split_data = []\n",
    "    split_labels = []\n",
    "    split_path = os.path.join(DATASET_DIR, split)\n",
    "\n",
    "    print(f\"\\nüìÇ Loading {split} data...\")\n",
    "\n",
    "    for category in CATEGORIES:\n",
    "        category_path = os.path.join(split_path, category)\n",
    "\n",
    "        # Loop through subfolders (e.g., apple rotten/, apple eaten/)\n",
    "        for subfolder in os.listdir(category_path):\n",
    "            subfolder_path = os.path.join(category_path, subfolder)\n",
    "            if not os.path.isdir(subfolder_path):\n",
    "                continue\n",
    "\n",
    "            for img_name in tqdm(os.listdir(subfolder_path), desc=f\"{split}/{category}/{subfolder}\"):\n",
    "                img_path = os.path.join(subfolder_path, img_name)\n",
    "                try:\n",
    "                    img = cv2.imread(img_path)\n",
    "                    if img is None:\n",
    "                        continue\n",
    "                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                    img = cv2.resize(img, IMG_SIZE)\n",
    "                    img = img.astype(np.float32) / 255.0\n",
    "\n",
    "                    split_data.append(img)\n",
    "                    split_labels.append(category)\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Error loading {img_path}: {e}\")\n",
    "\n",
    "    data_splits[split] = np.array(split_data)\n",
    "    labels_splits[split] = np.array(split_labels)\n",
    "\n",
    "    print(f\"‚úÖ {split} images loaded: {len(split_data)}\")\n",
    "    print(f\"üìê {split} shape: {data_splits[split].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fb9992",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Encode labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e90fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Fit encoder only once (using training labels)\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(labels_splits[\"training\"])\n",
    "\n",
    "# Encode all splits\n",
    "y_train = to_categorical(label_encoder.transform(labels_splits[\"training\"]))\n",
    "y_val = to_categorical(label_encoder.transform(labels_splits[\"validation\"]))\n",
    "y_test = to_categorical(label_encoder.transform(labels_splits[\"test\"]))\n",
    "\n",
    "X_train = data_splits[\"training\"]\n",
    "X_val = data_splits[\"validation\"]\n",
    "X_test = data_splits[\"test\"]\n",
    "\n",
    "print(\"\\nüè∑Ô∏è Label mapping:\")\n",
    "for i, label in enumerate(label_encoder.classes_):\n",
    "    print(f\"  {i}: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fb50e2",
   "metadata": {},
   "source": [
    "### Show Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bedfdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample_images(X, y, encoder, n=5):\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    for i in range(n):\n",
    "        idx = np.random.randint(0, len(X))\n",
    "        plt.subplot(1, n, i+1)\n",
    "        plt.imshow(X[idx])\n",
    "        plt.title(encoder.inverse_transform([np.argmax(y[idx])])[0])\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "show_sample_images(X_train, y_train, label_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9eeeea7",
   "metadata": {},
   "source": [
    "### Part 2: Data Augmentation\n",
    "\n",
    "After successfully loading and preprocessing our dataset in **Part 1**, the next steps are to:\n",
    "\n",
    "1. **Apply data augmentation** to make the model more robust and reduce overfitting.  \n",
    "2. **Prepare the data generators** that will feed the model efficiently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbbac65",
   "metadata": {},
   "source": [
    "### üì∏ Data Augmentation\n",
    "\n",
    "To make the model generalize better, we apply small random transformations to the training images:\n",
    "\n",
    "- **Rotation:** up to ¬±20¬∞  \n",
    "- **Shifting:** horizontally or vertically up to 10%  \n",
    "- **Zooming:** up to 20%  \n",
    "- **Horizontal flip:** mirrors images to simulate variations\n",
    "\n",
    "These augmentations are applied **on the fly** during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58a2d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: Data Augmentation\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Data augmentation for training set only\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator()  # no augmentation on validation data\n",
    "test_datagen = ImageDataGenerator()\n",
    "\n",
    "train_generator = train_datagen.flow(X_train, y_train, batch_size=32, shuffle=True)\n",
    "val_generator = val_datagen.flow(X_val, y_val, batch_size=32, shuffle=False)\n",
    "test_generator = test_datagen.flow(X_test, y_test, batch_size=32, shuffle=False)\n",
    "\n",
    "print(\"‚úÖ Data augmentation ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b45040",
   "metadata": {},
   "source": [
    "### Visualize Augmented Images (Optional)\n",
    "\n",
    "Before moving to model training, it‚Äôs a good idea to visualize a few augmented samples to ensure the transformations look correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a4548c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "augmented_images, _ = next(train_generator)\n",
    "plt.figure(figsize=(15, 3))\n",
    "for i in range(5):\n",
    "    plt.subplot(1, 5, i+1)\n",
    "    plt.imshow(augmented_images[i])\n",
    "    plt.axis(\"off\")\n",
    "plt.suptitle(\"Example Augmented Training Images\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abceca52",
   "metadata": {},
   "source": [
    "## Part 3 ‚Äî Model Building (Transfer Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df21b01",
   "metadata": {},
   "source": [
    "### Step 1: Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61b9af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b13bb8",
   "metadata": {},
   "source": [
    "### Step 2: Load the Pre-trained Model (ResNet50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa4dafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of classes\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Load pre-trained ResNet50\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9121db8d",
   "metadata": {},
   "source": [
    "### Step 3: Build the Classification Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34833459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new model on top of base\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9a99c5",
   "metadata": {},
   "source": [
    "### Step 4: Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b923c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877a921a",
   "metadata": {},
   "source": [
    "üìù Why categorical_crossentropy?\n",
    "Because your labels are one-hot encoded (y_train and y_test)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27223dfe",
   "metadata": {},
   "source": [
    "### Step 5: Preprocess and Prepare Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bff80f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "print(\"üîÑ Preprocessing training data...\")\n",
    "X_train_prep = preprocess_input(X_train)\n",
    "print(\"‚úÖ Training data preprocessing complete.\")\n",
    "\n",
    "print(\"üîÑ Preprocessing test data...\")\n",
    "X_test_prep = preprocess_input(X_test)\n",
    "print(\"‚úÖ Test data preprocessing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5271870c",
   "metadata": {},
   "source": [
    "### Step 6: Data Augmentation (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b636972",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=0.2\n",
    ")\n",
    "\n",
    "datagen.fit(X_train_prep)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1458260e",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3107a5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "\n",
    "# ‚úÖ Check if GPU is available\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    device_name = '/GPU:0'\n",
    "    print(f\"‚úÖ Training on GPU: {gpus[0]}\")\n",
    "else:\n",
    "    device_name = '/CPU:0'\n",
    "    print(\"‚ö†Ô∏è No GPU detected ‚Äî training on CPU.\")\n",
    "\n",
    "\n",
    "# Custom callback to measure time per epoch\n",
    "class TimeHistory(Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch_start_time = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        epoch_time = time.time() - self.epoch_start_time\n",
    "        print(f\"‚è±Ô∏è Time for epoch {epoch + 1}: {epoch_time:.2f} seconds\")\n",
    "\n",
    "# Instantiate the callback\n",
    "time_callback = TimeHistory()\n",
    "\n",
    "\n",
    "# Model training\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "\n",
    "with tf.device(device_name):\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        validation_data=val_generator,\n",
    "        epochs=epochs,\n",
    "        callbacks=[time_callback],\n",
    "        verbose=1       # Show progress bar with each epoch\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbfb4e4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d3e9058",
   "metadata": {},
   "source": [
    "‚úÖ More epochs ‚Üí the model learns more patterns.\n",
    "\n",
    "‚ö†Ô∏è But too many epochs can lead to overfitting ‚Äî when the model memorizes the training data and performs poorly on new data.\n",
    "\n",
    "Too few epochs can lead to underfitting ‚Äî when the model hasn‚Äôt learned enough.\n",
    "\n",
    "üìù Tip: Start with 10‚Äì20 epochs, then increase if the model is still improving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59c2792",
   "metadata": {},
   "source": [
    "## Part 4: Model Evaluation and Visualization\n",
    "After training the model, it‚Äôs important to evaluate its performance and visualize how well it learned over time. This helps us determine if the model is underfitting, overfitting, or performing as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79ae1ce",
   "metadata": {},
   "source": [
    "### üìä 1. Evaluating the Model\n",
    "\n",
    "We use the test dataset to check how well the model performs on unseen data.\n",
    "This gives us metrics like accuracy and loss, which reflect how close the model‚Äôs predictions are to the actual labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddae3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on test data\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(f\"‚úÖ Test Accuracy: {test_acc*100:.2f}%\")\n",
    "print(f\"üìâ Test Loss: {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5810a3",
   "metadata": {},
   "source": [
    "### Plotting Training Accuracy and Loss\n",
    "\n",
    "During training, we stored the accuracy and loss values for each epoch.\n",
    "Plotting them helps us visualize how the model improved over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a24fe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot accuracy\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc5f145",
   "metadata": {},
   "source": [
    "üìù Interpretation Tips:\n",
    "\n",
    "üìà If training accuracy is much higher than validation accuracy ‚Üí overfitting.\n",
    "\n",
    "üìâ If both are low ‚Üí underfitting.\n",
    "\n",
    "‚úÖ If both curves improve smoothly and are close ‚Üí good training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea41fcfb",
   "metadata": {},
   "source": [
    "### 3. Confusion Matrix\n",
    "\n",
    "A confusion matrix provides a more detailed view of classification results.\n",
    "It shows how many times the model correctly predicted each class versus how many times it confused it with another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1fa4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Get predictions on test set\n",
    "y_true = np.argmax(y_test, axis=1)  # true labels from encoded test set\n",
    "y_pred_probs = model.predict(test_generator)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_encoder.classes_)\n",
    "disp.plot(cmap='Blues', xticks_rotation=45)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584e05a9",
   "metadata": {},
   "source": [
    "üß† How to read the confusion matrix:\n",
    "\n",
    "Diagonal values = correct predictions ‚úÖ\n",
    "\n",
    "Off-diagonal values = misclassifications ‚ùå\n",
    "\n",
    "A perfect model would have non-zero values only along the diagonal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
