{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd6ffd63",
   "metadata": {},
   "source": [
    "## Fruit Image Classifier - Part 1: Imports & Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4085a153",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrandom\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moptim\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\__init__.py:2126\u001b[39m\n\u001b[32m   2119\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_compile\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _disable_dynamo  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[32m   2121\u001b[39m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[32m   2122\u001b[39m \u001b[38;5;66;03m# Import interface functions defined in Python\u001b[39;00m\n\u001b[32m   2123\u001b[39m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[32m   2124\u001b[39m \n\u001b[32m   2125\u001b[39m \u001b[38;5;66;03m# needs to be after the above ATen bindings so we can overwrite from Python side\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2126\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _VF \u001b[38;5;28;01mas\u001b[39;00m _VF, functional \u001b[38;5;28;01mas\u001b[39;00m functional  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[32m   2127\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# usort: skip # noqa: F403\u001b[39;00m\n\u001b[32m   2129\u001b[39m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[32m   2130\u001b[39m \u001b[38;5;66;03m# Remove unnecessary members\u001b[39;00m\n\u001b[32m   2131\u001b[39m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\functional.py:8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Optional, TYPE_CHECKING, Union\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mF\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _VF, Tensor\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_C\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _add_docstr\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\__init__.py:8\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# mypy: allow-untyped-defs\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparameter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[32m      3\u001b[39m     Buffer \u001b[38;5;28;01mas\u001b[39;00m Buffer,\n\u001b[32m      4\u001b[39m     Parameter \u001b[38;5;28;01mas\u001b[39;00m Parameter,\n\u001b[32m      5\u001b[39m     UninitializedBuffer \u001b[38;5;28;01mas\u001b[39;00m UninitializedBuffer,\n\u001b[32m      6\u001b[39m     UninitializedParameter \u001b[38;5;28;01mas\u001b[39;00m UninitializedParameter,\n\u001b[32m      7\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodules\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# usort: skip # noqa: F403\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     10\u001b[39m     attention \u001b[38;5;28;01mas\u001b[39;00m attention,\n\u001b[32m     11\u001b[39m     functional \u001b[38;5;28;01mas\u001b[39;00m functional,\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m     utils \u001b[38;5;28;01mas\u001b[39;00m utils,\n\u001b[32m     17\u001b[39m )\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparallel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataParallel \u001b[38;5;28;01mas\u001b[39;00m DataParallel\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\__init__.py:53\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchannelshuffle\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChannelShuffle\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcontainer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     46\u001b[39m     Container,\n\u001b[32m     47\u001b[39m     ModuleDict,\n\u001b[32m   (...)\u001b[39m\u001b[32m     51\u001b[39m     Sequential,\n\u001b[32m     52\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     54\u001b[39m     Conv1d,\n\u001b[32m     55\u001b[39m     Conv2d,\n\u001b[32m     56\u001b[39m     Conv3d,\n\u001b[32m     57\u001b[39m     ConvTranspose1d,\n\u001b[32m     58\u001b[39m     ConvTranspose2d,\n\u001b[32m     59\u001b[39m     ConvTranspose3d,\n\u001b[32m     60\u001b[39m     LazyConv1d,\n\u001b[32m     61\u001b[39m     LazyConv2d,\n\u001b[32m     62\u001b[39m     LazyConv3d,\n\u001b[32m     63\u001b[39m     LazyConvTranspose1d,\n\u001b[32m     64\u001b[39m     LazyConvTranspose2d,\n\u001b[32m     65\u001b[39m     LazyConvTranspose3d,\n\u001b[32m     66\u001b[39m )\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistance\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CosineSimilarity, PairwiseDistance\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdropout\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     69\u001b[39m     AlphaDropout,\n\u001b[32m     70\u001b[39m     Dropout,\n\u001b[32m   (...)\u001b[39m\u001b[32m     74\u001b[39m     FeatureAlphaDropout,\n\u001b[32m     75\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1331\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:935\u001b[39m, in \u001b[36m_load_unlocked\u001b[39m\u001b[34m(spec)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1022\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1118\u001b[39m, in \u001b[36mget_code\u001b[39m\u001b[34m(self, fullname)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1217\u001b[39m, in \u001b[36mget_data\u001b[39m\u001b[34m(self, path)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Fix random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# src imports\n",
    "from gpu_utils import CheckGPU, CheckCUDA, CheckGPUBrief\n",
    "from dataset_counter import CountDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f3294f",
   "metadata": {},
   "source": [
    "#### Detect GPU Available, Details, Cuda, and cuDNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25d3114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "🖥️  GPU INFORMATION\n",
      "==================================================\n",
      "✅ GPU Detected         : NVIDIA GeForce RTX 2060\n",
      "   • Device ID          : 0\n",
      "   • Compute Capability : 7.5\n",
      "   • Multiprocessors    : 30\n",
      "   • Total VRAM         : 6.00 GB\n",
      "   • VRAM Allocated     : 0.00 GB\n",
      "   • VRAM Reserved      : 0.00 GB\n",
      "   • Active Device      : cuda\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "⚡ CUDA / PYTORCH INFORMATION\n",
      "==================================================\n",
      "✅ CUDA Available       : True\n",
      "   • PyTorch CUDA Ver.  : 11.8\n",
      "   • PyTorch Version    : 2.7.1+cu118\n",
      "✅ cuDNN Version        : 90100\n",
      "   • CUDA Device Count  : 1\n",
      "   • Device 0 Name     : NVIDIA GeForce RTX 2060\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# From \n",
    "CheckGPU()\n",
    "CheckCUDA()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e219a799",
   "metadata": {},
   "source": [
    "### Define dataset path and categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac14c32",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CountDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m DATASET_DIR = \u001b[33m\"\u001b[39m\u001b[33m../dataset\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m DATASET_INFO = \u001b[43mCountDataset\u001b[49m(DATASET_DIR)\n\u001b[32m      4\u001b[39m \u001b[33;03m'''\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33;03m# Example:\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[33;03mDATASET_INFO = {\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m \u001b[33;03m}\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[33;03m'''\u001b[39;00m\n\u001b[32m     20\u001b[39m IMG_SIZE = \u001b[32m224\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'CountDataset' is not defined"
     ]
    }
   ],
   "source": [
    "DATASET_DIR = \"../dataset\"\n",
    "\n",
    "from dataset_counter import CountDataset\n",
    "DATASET_INFO = CountDataset(DATASET_DIR)\n",
    "'''\n",
    "# Example:\n",
    "DATASET_INFO = {\n",
    "    \"Apple\":  {\"count\": 19515, \"ratio\":  1.0000, \"needed\":     0, \"size_mb\":  816.94},\n",
    "    \"Banana\": {\"count\": 11612, \"ratio\":  1.6806, \"needed\":  7903, \"size_mb\":  223.78},\n",
    "    \"Grapes\": {\"count\":  2198, \"ratio\":  8.8785, \"needed\": 17317, \"size_mb\": 6605.82},\n",
    "    \"Mango\":  {\"count\":  2505, \"ratio\":  7.7904, \"needed\": 17010, \"size_mb\":  191.21},\n",
    "    \"Orange\": {\"count\":   232, \"ratio\": 84.1164, \"needed\": 19283, \"size_mb\":   29.25},\n",
    "\n",
    "    \"total_images\": 36062,\n",
    "    \"max_class_count\": 19515,\n",
    "    \"total_size_mb\": 7867.01\n",
    "}\n",
    "'''\n",
    "\n",
    "\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fb9992",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Encode labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e90fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Fit encoder only once (using training labels)\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(labels_splits[\"training\"])\n",
    "\n",
    "# Encode all splits\n",
    "y_train = to_categorical(label_encoder.transform(labels_splits[\"training\"]))\n",
    "y_val = to_categorical(label_encoder.transform(labels_splits[\"validation\"]))\n",
    "y_test = to_categorical(label_encoder.transform(labels_splits[\"test\"]))\n",
    "\n",
    "X_train = data_splits[\"training\"]\n",
    "X_val = data_splits[\"validation\"]\n",
    "X_test = data_splits[\"test\"]\n",
    "\n",
    "print(\"\\n🏷️ Label mapping:\")\n",
    "for i, label in enumerate(label_encoder.classes_):\n",
    "    print(f\"  {i}: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fb50e2",
   "metadata": {},
   "source": [
    "### Show Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bedfdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample_images(X, y, encoder, n=5):\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    for i in range(n):\n",
    "        idx = np.random.randint(0, len(X))\n",
    "        plt.subplot(1, n, i+1)\n",
    "        plt.imshow(X[idx])\n",
    "        plt.title(encoder.inverse_transform([np.argmax(y[idx])])[0])\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "show_sample_images(X_train, y_train, label_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9eeeea7",
   "metadata": {},
   "source": [
    "### Part 2: Data Augmentation\n",
    "\n",
    "After successfully loading and preprocessing our dataset in **Part 1**, the next steps are to:\n",
    "\n",
    "1. **Apply data augmentation** to make the model more robust and reduce overfitting.  \n",
    "2. **Prepare the data generators** that will feed the model efficiently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbbac65",
   "metadata": {},
   "source": [
    "### 📸 Data Augmentation\n",
    "\n",
    "To make the model generalize better, we apply small random transformations to the training images:\n",
    "\n",
    "- **Rotation:** up to ±20°  \n",
    "- **Shifting:** horizontally or vertically up to 10%  \n",
    "- **Zooming:** up to 20%  \n",
    "- **Horizontal flip:** mirrors images to simulate variations\n",
    "\n",
    "These augmentations are applied **on the fly** during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58a2d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: Data Augmentation\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Data augmentation for training set only\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator()  # no augmentation on validation data\n",
    "test_datagen = ImageDataGenerator()\n",
    "\n",
    "train_generator = train_datagen.flow(X_train, y_train, batch_size=32, shuffle=True)\n",
    "val_generator = val_datagen.flow(X_val, y_val, batch_size=32, shuffle=False)\n",
    "test_generator = test_datagen.flow(X_test, y_test, batch_size=32, shuffle=False)\n",
    "\n",
    "print(\"✅ Data augmentation ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b45040",
   "metadata": {},
   "source": [
    "### Visualize Augmented Images (Optional)\n",
    "\n",
    "Before moving to model training, it’s a good idea to visualize a few augmented samples to ensure the transformations look correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a4548c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "augmented_images, _ = next(train_generator)\n",
    "plt.figure(figsize=(15, 3))\n",
    "for i in range(5):\n",
    "    plt.subplot(1, 5, i+1)\n",
    "    plt.imshow(augmented_images[i])\n",
    "    plt.axis(\"off\")\n",
    "plt.suptitle(\"Example Augmented Training Images\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abceca52",
   "metadata": {},
   "source": [
    "## Part 3 — Model Building (Transfer Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df21b01",
   "metadata": {},
   "source": [
    "### Step 1: Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61b9af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b13bb8",
   "metadata": {},
   "source": [
    "### Step 2: Load the Pre-trained Model (ResNet50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa4dafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of classes\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Load pre-trained ResNet50\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9121db8d",
   "metadata": {},
   "source": [
    "### Step 3: Build the Classification Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34833459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new model on top of base\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9a99c5",
   "metadata": {},
   "source": [
    "### Step 4: Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b923c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877a921a",
   "metadata": {},
   "source": [
    "📝 Why categorical_crossentropy?\n",
    "Because your labels are one-hot encoded (y_train and y_test)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27223dfe",
   "metadata": {},
   "source": [
    "### Step 5: Preprocess and Prepare Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bff80f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "print(\"🔄 Preprocessing training data...\")\n",
    "X_train_prep = preprocess_input(X_train)\n",
    "print(\"✅ Training data preprocessing complete.\")\n",
    "\n",
    "print(\"🔄 Preprocessing test data...\")\n",
    "X_test_prep = preprocess_input(X_test)\n",
    "print(\"✅ Test data preprocessing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5271870c",
   "metadata": {},
   "source": [
    "### Step 6: Data Augmentation (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b636972",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=0.2\n",
    ")\n",
    "\n",
    "datagen.fit(X_train_prep)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1458260e",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3107a5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "\n",
    "# ✅ Check if GPU is available\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    device_name = '/GPU:0'\n",
    "    print(f\"✅ Training on GPU: {gpus[0]}\")\n",
    "else:\n",
    "    device_name = '/CPU:0'\n",
    "    print(\"⚠️ No GPU detected — training on CPU.\")\n",
    "\n",
    "\n",
    "# Custom callback to measure time per epoch\n",
    "class TimeHistory(Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch_start_time = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        epoch_time = time.time() - self.epoch_start_time\n",
    "        print(f\"⏱️ Time for epoch {epoch + 1}: {epoch_time:.2f} seconds\")\n",
    "\n",
    "# Instantiate the callback\n",
    "time_callback = TimeHistory()\n",
    "\n",
    "\n",
    "# Model training\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "\n",
    "with tf.device(device_name):\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        validation_data=val_generator,\n",
    "        epochs=epochs,\n",
    "        callbacks=[time_callback],\n",
    "        verbose=1       # Show progress bar with each epoch\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbfb4e4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d3e9058",
   "metadata": {},
   "source": [
    "✅ More epochs → the model learns more patterns.\n",
    "\n",
    "⚠️ But too many epochs can lead to overfitting — when the model memorizes the training data and performs poorly on new data.\n",
    "\n",
    "Too few epochs can lead to underfitting — when the model hasn’t learned enough.\n",
    "\n",
    "📝 Tip: Start with 10–20 epochs, then increase if the model is still improving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59c2792",
   "metadata": {},
   "source": [
    "## Part 4: Model Evaluation and Visualization\n",
    "After training the model, it’s important to evaluate its performance and visualize how well it learned over time. This helps us determine if the model is underfitting, overfitting, or performing as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79ae1ce",
   "metadata": {},
   "source": [
    "### 📊 1. Evaluating the Model\n",
    "\n",
    "We use the test dataset to check how well the model performs on unseen data.\n",
    "This gives us metrics like accuracy and loss, which reflect how close the model’s predictions are to the actual labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddae3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on test data\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(f\"✅ Test Accuracy: {test_acc*100:.2f}%\")\n",
    "print(f\"📉 Test Loss: {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5810a3",
   "metadata": {},
   "source": [
    "### Plotting Training Accuracy and Loss\n",
    "\n",
    "During training, we stored the accuracy and loss values for each epoch.\n",
    "Plotting them helps us visualize how the model improved over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a24fe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot accuracy\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc5f145",
   "metadata": {},
   "source": [
    "📝 Interpretation Tips:\n",
    "\n",
    "📈 If training accuracy is much higher than validation accuracy → overfitting.\n",
    "\n",
    "📉 If both are low → underfitting.\n",
    "\n",
    "✅ If both curves improve smoothly and are close → good training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea41fcfb",
   "metadata": {},
   "source": [
    "### 3. Confusion Matrix\n",
    "\n",
    "A confusion matrix provides a more detailed view of classification results.\n",
    "It shows how many times the model correctly predicted each class versus how many times it confused it with another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1fa4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Get predictions on test set\n",
    "y_true = np.argmax(y_test, axis=1)  # true labels from encoded test set\n",
    "y_pred_probs = model.predict(test_generator)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_encoder.classes_)\n",
    "disp.plot(cmap='Blues', xticks_rotation=45)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584e05a9",
   "metadata": {},
   "source": [
    "🧠 How to read the confusion matrix:\n",
    "\n",
    "Diagonal values = correct predictions ✅\n",
    "\n",
    "Off-diagonal values = misclassifications ❌\n",
    "\n",
    "A perfect model would have non-zero values only along the diagonal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
